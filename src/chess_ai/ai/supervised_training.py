"""
Entra√Ænement supervis√© avec Stockfish
====================================

Utilise Stockfish comme "professeur" pour pr√©-entra√Æner le r√©seau neuronal
avant l'auto-jeu AlphaZero. Cela acc√©l√®re consid√©rablement l'apprentissage.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import chess
import chess.pgn
import numpy as np
import random
from typing import List, Tuple, Optional
import logging
from tqdm import tqdm
import os
import json

from ..ai.network import ChessNet, encode_board, decode_policy
from ..ai.reference_evaluator import get_reference_evaluator

logger = logging.getLogger(__name__)


class SupervisedTrainer:
    """
    Entra√Æneur supervis√© utilisant Stockfish comme r√©f√©rence.

    Ce module g√©n√®re des positions al√©atoires ou utilise des parties PGN,
    puis les √©value avec Stockfish pour cr√©er des donn√©es d'entra√Ænement
    supervis√© pour le r√©seau neuronal.
    """

    def __init__(
        self,
        model: ChessNet,
        device: str = "cpu",
        learning_rate: float = 0.001,
        stockfish_depth: int = 10,
    ):
        """
        Initialise l'entra√Æneur supervis√©.

        Args:
            model: R√©seau neuronal √† entra√Æner
            device: Device PyTorch (cpu/cuda)
            learning_rate: Taux d'apprentissage
            stockfish_depth: Profondeur d'analyse Stockfish
        """
        self.model = model
        self.device = device
        self.model.to(device)

        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.value_criterion = nn.MSELoss()
        self.policy_criterion = nn.CrossEntropyLoss()

        # √âvaluateur de r√©f√©rence
        self.evaluator = get_reference_evaluator()
        if hasattr(self.evaluator, "depth"):
            self.evaluator.depth = stockfish_depth

        self.training_history = []

    def generate_random_positions(self, num_positions: int = 1000) -> List[chess.Board]:
        """
        G√©n√®re des positions d'√©checs al√©atoires mais plausibles.

        Args:
            num_positions: Nombre de positions √† g√©n√©rer

        Returns:
            Liste de positions d'√©checs
        """
        positions = []

        for _ in tqdm(range(num_positions), desc="G√©n√©ration de positions"):
            board = chess.Board()

            # Jouer un nombre al√©atoire de coups (5-50)
            num_moves = random.randint(5, 50)

            for _ in range(num_moves):
                if board.is_game_over():
                    break

                legal_moves = list(board.legal_moves)
                if not legal_moves:
                    break

                # Jouer un coup al√©atoire
                move = random.choice(legal_moves)
                board.push(move)

            # Garder seulement les positions non terminales
            if not board.is_game_over():
                positions.append(board.copy())

        return positions

    def load_pgn_positions(
        self, pgn_file: str, max_positions: int = 1000
    ) -> List[chess.Board]:
        """
        Charge des positions depuis un fichier PGN.

        Args:
            pgn_file: Chemin vers le fichier PGN
            max_positions: Nombre maximum de positions √† extraire

        Returns:
            Liste de positions d'√©checs
        """
        positions = []

        try:
            with open(pgn_file, "r", encoding="utf-8") as f:
                while len(positions) < max_positions:
                    game = chess.pgn.read_game(f)
                    if game is None:
                        break

                    board = game.board()
                    for move in game.mainline_moves():
                        board.push(move)

                        # Prendre quelques positions de la partie
                        if len(board.move_stack) > 10 and random.random() < 0.1:
                            if not board.is_game_over():
                                positions.append(board.copy())

                        if len(positions) >= max_positions:
                            break

                    if len(positions) >= max_positions:
                        break

        except FileNotFoundError:
            logger.warning(f"Fichier PGN non trouv√© : {pgn_file}")
        except Exception as e:
            logger.error(f"Erreur lors du chargement PGN : {e}")

        return positions

    def create_training_data(
        self, positions: List[chess.Board], batch_size: int = 32
    ) -> List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:
        """
        Cr√©e les donn√©es d'entra√Ænement √† partir des positions.

        Args:
            positions: Positions d'√©checs √† √©valuer
            batch_size: Taille des batches

        Returns:
            Liste de (position_encod√©e, politique_cible, valeur_cible)
        """
        training_data = []

        for i in tqdm(range(0, len(positions), batch_size), desc="Cr√©ation donn√©es"):
            batch_positions = positions[i : i + batch_size]
            batch_encoded = []
            batch_values = []
            batch_policies = []

            for board in batch_positions:
                try:
                    # Encoder la position
                    encoded = encode_board(board)

                    # √âvaluation Stockfish
                    value = self.evaluator.evaluate_position(board)

                    # Politique cible : meilleur coup Stockfish
                    best_move = self.evaluator.get_best_move(board)
                    policy_target = torch.zeros(4672)  # Taille de l'espace d'action

                    if best_move and best_move in board.legal_moves:
                        # Cr√©er une distribution concentr√©e sur le meilleur coup
                        legal_moves = list(board.legal_moves)
                        move_probs = decode_policy(policy_target, legal_moves)

                        # Politique concentr√©e (80% meilleur coup, 20% autres)
                        for i, move in enumerate(legal_moves):
                            if move == best_move:
                                move_probs[move] = 0.8
                            else:
                                move_probs[move] = 0.2 / (len(legal_moves) - 1)

                        # Reconvertir en logits
                        for move, prob in move_probs.items():
                            # Ici il faudrait un encodage move->index
                            # Simplifi√© pour l'exemple
                            pass

                    batch_encoded.append(encoded)
                    batch_values.append(value)
                    batch_policies.append(policy_target)

                except Exception as e:
                    logger.warning(f"Erreur traitement position : {e}")
                    continue

            if batch_encoded:
                batch_tensor = torch.stack(batch_encoded)
                value_tensor = torch.tensor(batch_values, dtype=torch.float32)
                policy_tensor = torch.stack(batch_policies)

                training_data.append((batch_tensor, policy_tensor, value_tensor))

        return training_data

    def train_epoch(
        self, training_data: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]
    ) -> dict:
        """
        Entra√Æne le mod√®le sur une √©poque.

        Args:
            training_data: Donn√©es d'entra√Ænement

        Returns:
            Statistiques d'entra√Ænement
        """
        self.model.train()
        total_loss = 0.0
        total_value_loss = 0.0
        total_policy_loss = 0.0
        num_batches = 0

        for batch_positions, batch_policies, batch_values in tqdm(
            training_data, desc="Entra√Ænement"
        ):
            batch_positions = batch_positions.to(self.device)
            batch_policies = batch_policies.to(self.device)
            batch_values = batch_values.to(self.device).unsqueeze(1)

            # Forward pass
            policy_logits, predicted_values = self.model(batch_positions)

            # Calcul des pertes
            value_loss = self.value_criterion(predicted_values, batch_values)

            # Pour la politique, on utilise KL divergence ou cross-entropy
            policy_loss = self.policy_criterion(
                policy_logits, batch_policies.argmax(dim=1)
            )

            total_loss_batch = value_loss + policy_loss

            # Backward pass
            self.optimizer.zero_grad()
            total_loss_batch.backward()
            self.optimizer.step()

            # Statistiques
            total_loss += total_loss_batch.item()
            total_value_loss += value_loss.item()
            total_policy_loss += policy_loss.item()
            num_batches += 1

        stats = {
            "total_loss": total_loss / num_batches,
            "value_loss": total_value_loss / num_batches,
            "policy_loss": total_policy_loss / num_batches,
            "num_batches": num_batches,
        }

        return stats

    def pretrain(
        self,
        num_positions: int = 5000,
        epochs: int = 10,
        pgn_file: Optional[str] = None,
        save_path: Optional[str] = None,
    ) -> dict:
        """
        Pr√©-entra√Ænement supervis√© complet.

        Args:
            num_positions: Nombre de positions √† g√©n√©rer/utiliser
            epochs: Nombre d'√©poques d'entra√Ænement
            pgn_file: Fichier PGN optionnel
            save_path: Chemin de sauvegarde du mod√®le

        Returns:
            Historique d'entra√Ænement
        """
        print(f"üß† D√©marrage pr√©-entra√Ænement supervis√© avec Stockfish")
        print(f"üìä {num_positions} positions, {epochs} √©poques")

        # 1. G√©n√©rer/charger positions
        if pgn_file and os.path.exists(pgn_file):
            print(f"üìÅ Chargement positions depuis {pgn_file}")
            positions = self.load_pgn_positions(pgn_file, num_positions)
        else:
            print("üé≤ G√©n√©ration de positions al√©atoires")
            positions = self.generate_random_positions(num_positions)

        if not positions:
            raise ValueError("Aucune position g√©n√©r√©e")

        print(f"‚úÖ {len(positions)} positions pr√™tes")

        # 2. Cr√©er donn√©es d'entra√Ænement
        print("üîÑ √âvaluation positions avec Stockfish...")
        training_data = self.create_training_data(positions)

        if not training_data:
            raise ValueError("Aucune donn√©e d'entra√Ænement cr√©√©e")

        print(f"‚úÖ {len(training_data)} batches de donn√©es cr√©√©s")

        # 3. Entra√Ænement
        training_history = []

        for epoch in range(epochs):
            print(f"\nüìà √âpoque {epoch + 1}/{epochs}")

            # M√©langer les donn√©es
            random.shuffle(training_data)

            # Entra√Æner
            stats = self.train_epoch(training_data)
            stats["epoch"] = epoch + 1
            training_history.append(stats)

            print(f"üíî Perte totale: {stats['total_loss']:.4f}")
            print(f"üìä Perte valeur: {stats['value_loss']:.4f}")
            print(f"üéØ Perte politique: {stats['policy_loss']:.4f}")

        # 4. Sauvegarde
        if save_path:
            torch.save(
                {
                    "model_state_dict": self.model.state_dict(),
                    "optimizer_state_dict": self.optimizer.state_dict(),
                    "training_history": training_history,
                    "supervised_pretrained": True,
                },
                save_path,
            )
            print(f"üíæ Mod√®le sauvegard√© : {save_path}")

        self.training_history.extend(training_history)

        print(f"üéâ Pr√©-entra√Ænement termin√© !")
        print(f"üöÄ Le r√©seau est maintenant pr√©-entra√Æn√© avec Stockfish")

        return {
            "history": training_history,
            "final_stats": training_history[-1] if training_history else {},
            "num_positions": len(positions),
            "num_batches": len(training_data),
        }


def create_supervised_trainer(model_path: Optional[str] = None) -> SupervisedTrainer:
    """
    Cr√©e un entra√Æneur supervis√© avec un mod√®le.

    Args:
        model_path: Chemin vers un mod√®le existant (optionnel)

    Returns:
        Entra√Æneur supervis√© configur√©
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = ChessNet().to(device)

    if model_path and os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        print(f"üìÅ Mod√®le charg√© depuis {model_path}")

    return SupervisedTrainer(model, device=device)
